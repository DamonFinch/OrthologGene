{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19ff2b5",
   "metadata": {},
   "source": [
    "# Jupyter interface of OrthoSLC (0.1)\n",
    "\n",
    "<b>OrthoSLC</b> is a pipline that perfomrs Reciprocal Best Blast Hit (RBBH) Single Linkage Clustering to obtain Orthologous Genes which assist core genome construction. <br>\n",
    "\n",
    "**This interface** is python jupyter based to facilitate customized analysis. However, for datasets with 500 or more genomes, we still recommend using the binary files, which is mainly written in C++, for optimal performance.<br>\n",
    "\n",
    "It is: <br>\n",
    "* <b>independent</b> of relational database management systems (e.g., MySQL)\n",
    "* recommend to handle less genomes.\n",
    "\n",
    "It is recommended for sub-species level single copy core genome construction since RBBH may not work well for missions like Human-Microbe core genome construction. \n",
    "\n",
    "**Version change**:<br>\n",
    "`0.1Alpha` -> `0.1Beta`:<br>\n",
    "* allow users to set `bin level`, in steps using hash binning, with a linear style instead of exponatial style.<br>\n",
    "\n",
    "* **Fixed**: fixed output path concatenation failure in Step 2.\n",
    "\n",
    "`0.1Beta` -> current version `0.1`:<br>\n",
    "* applied pre-clustering before BLAST which significantly reduce time usage especially when phylogenetically close genomes paticipated analysis.\n",
    "    * 500 <i>Listeria monocytogenes</i> in `0.1Beta` -> ~7 hours, `0.1` -> ~22 mins\n",
    "    * 1150 <i>E. coli</i> `0.1Beta` -> ~2.7 days, `0.1` -> less than 5 hours\n",
    "    \n",
    "* In `Step5_reprocal_blast`, we provide optinal memory efficient mode. It is a simple set up of multithreading when running BLAST. Since after pre clustering, some non-redundant genomes may become very small in size, which make them less efficient to BLAST with multiple threads. With memory efficient mode `off`, program will assign only 1 thread on each BLAST task so that there will be no thread waiting for database formatting and pre-processing. It can significantly increase the BLAST efficiency but with some more consumed memory.\n",
    "\n",
    "* we provide **no lock mode** in all steps that apply hash binning to speed up the process. We allow users to turn off mutex lock which is to safely write into files when multi-threading. In ours tests, program can generate files without data corruption when multi-threading with no lock (data corruption were rarely observed, the possiblity of data corruption may vary between conpution platform).\n",
    "\n",
    "* **Fixed**: fixed the bug happening when writing final clusteres of FASTA files.\n",
    "\n",
    "<b>Caveat:</b><br>\n",
    "The pipeline is currently only tested on Ubuntu 20.04 and 18.04.\n",
    "\n",
    "<b>Requirement:</b><br>\n",
    "Python3 (suggest newest stable release or higher),<br>\n",
    "NCBI Blast+ (suggest 2.12 or higher) <br>\n",
    "\n",
    "<b>Note:</b><br>\n",
    "For all steps, users do not need to make the ourput directory manually, program will do that for you.\n",
    "\n",
    "Bug report: \n",
    "* <jingjie.chencharly@gmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe38de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Lock, Process\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# pip install biopython\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f6a5d",
   "metadata": {},
   "source": [
    "## Step 1 Genome information preparation\n",
    "The pipeline starts with annotated genomes in fasta format. FASTA file name require strain name and extension (e.g., `strain_A.ffn`, `strain_B.fna`, `strain_C.fasta` etc.).<br> \n",
    "Step 1 needs the **path to directory of annotated FASTA files** as input, to genereate a header less, tab separated table, in which the \n",
    "* first column is a short ID, \n",
    "* second column is the strain name, \n",
    "* third column as the absolute path. \n",
    "\n",
    "The short ID of each genome is generated to save computational resources and storage space. Since reciprocal BLAST generates a large volume of files (millions to billions of rows if large number of genomes participated), each row contains the names of the query and subject. If the user provides input FASTA file names like:\n",
    "\n",
    "* `GCA_900627445.1_PFR31F05_genomic.fasta`\n",
    "* `GCA_021980615.1_PDT001237823.1_genomic.fasta`\n",
    "\n",
    "and such file names become part of gene identifier instead of the short ID used in this program, large size of additional storage will be consumed for intermediate files and even more pressure on computing memory for analysis of 1000~ genomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f870c1",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `raw_annotated_dir_path` to the path of directory of annoated FASTA files\n",
    "* Set variable `preparation_output_path` to the path of output tab separated files.<br>\n",
    "Then run the codes below [Step 1 execution](###step-1-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# directory path of input fastas\n",
    "raw_annotated_dir_path = '/data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/tests/'\n",
    "# path of output tsv\n",
    "preparation_output_path = 'test_op/S1_pre.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c777e17",
   "metadata": {},
   "source": [
    "### Step 1 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796e713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_c = 10000 \n",
    "\n",
    "with open(preparation_output_path, 'w') as pre_op:\n",
    "    for file_naam in os.listdir(raw_annotated_dir_path):\n",
    "        abs_path = os.path.join(raw_annotated_dir_path,\n",
    "                                file_naam\n",
    "                               )\n",
    "        \n",
    "        strain_naam = os.path.basename(abs_path)\n",
    "        strain_naam = os.path.splitext(strain_naam)[0]\n",
    "        \n",
    "        pre_op.write(str(start_c) \n",
    "                     + '\\t'\n",
    "                     + strain_naam\n",
    "                     + '\\t'\n",
    "                     + abs_path\n",
    "                     + '\\n'\n",
    "                    )\n",
    "        start_c += 1\n",
    "        \n",
    "pre_op.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74db3a",
   "metadata": {},
   "source": [
    "## Step2 FASTA dereplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0c565",
   "metadata": {},
   "source": [
    "Step 2 is to remove potential sequence duplication (e.g., copies of tRNA, some cds) within each genome. This dereplication is equivalent to 100% clustering, to obtain single copy.<br>\n",
    "Step 2 **requires the tab separated table output by Step 1 as input**, and specifying a directory for dereplicated files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27499a",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05e23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundancy_rm_a_file(path_n_id_tup, # abs path of an input fasta\n",
    "                         rred_folder, # path to 100% clustered result\n",
    "                        ):\n",
    "    \n",
    "    in_fasta = SeqIO.parse(path_n_id_tup[0],\n",
    "                           'fasta'\n",
    "                          )\n",
    "    \n",
    "    strain_naam = os.path.basename(path_n_id_tup[0])\n",
    "    strain_naam = os.path.splitext(strain_naam)[0]\n",
    "    \n",
    "    short_id = path_n_id_tup[1]\n",
    "    \n",
    "    added_seq = set()\n",
    "    to_save = []\n",
    "    gene_id_c = 10000\n",
    "    \n",
    "    for SEQ in in_fasta:\n",
    "        if str(SEQ.seq) in added_seq:\n",
    "            continue\n",
    "        else:\n",
    "            to_save.append(SeqRecord(seq = SEQ.seq,\n",
    "                                     id = short_id + '-' + str(gene_id_c),\n",
    "                                     description = SEQ.description\n",
    "                                    )\n",
    "                          )\n",
    "            gene_id_c += 1\n",
    "            added_seq |= set([str(SEQ.seq)\n",
    "                             ] # set a list\n",
    "                            ) # add to set\n",
    "    \n",
    "    SeqIO.write(to_save, \n",
    "                os.path.join(os.path.abspath(rred_folder), \n",
    "                             strain_naam + '.fasta'\n",
    "                            ),\n",
    "                'fasta')\n",
    "\n",
    "def redundancy_rm_files(m_lst,\n",
    "                        rred_folder_\n",
    "                       ):\n",
    "    \n",
    "    for ab_fp in m_lst:\n",
    "        redundancy_rm_a_file(ab_fp, # to provide abs path\n",
    "                             rred_folder_\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fbca2",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `preparation_output_path` to the path of output tab separated files.\n",
    "* Set variable `dereped_dir_path` to the path to directory of dereplicated files.\n",
    "* Set variable `process_number` as thread number.<br>\n",
    "Then run the codes below [Step 2 execution](###step-2-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c850853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# path of output tsv from Step1\n",
    "preparation_output_path = 'test_op/S1_pre.txt'\n",
    "# directory path of output\n",
    "dereped_dir_path = 'test_op/dereped'\n",
    "# how many jobs to parallel\n",
    "process_number = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71addc",
   "metadata": {},
   "source": [
    "### Step 2 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0bf28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 19 09:21:48 2023\n",
      "Thu Jan 19 09:21:49 2023\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # mkdir or not\n",
    "    if os.path.exists(dereped_dir_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(dereped_dir_path)\n",
    "        \n",
    "    # read short id\n",
    "    strain_short_pth_n_id = []\n",
    "    \n",
    "    with open(preparation_output_path, 'r') as pre_op:\n",
    "        rls = pre_op.readlines()\n",
    "        \n",
    "        for l in rls:\n",
    "            l_ = l.split('\\t')\n",
    "            strain_short_pth_n_id.append((l_[2][0: -1], # key remove \\n\n",
    "                                          l_[0]\n",
    "                                         )\n",
    "                                        )\n",
    "        \n",
    "    pre_op.close()\n",
    "    \n",
    "    mission_lst = list(np.array_split(strain_short_pth_n_id, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    # mp\n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "    \n",
    "        p = Process(target = redundancy_rm_files,\n",
    "                    args = (sub_mission_lst, \n",
    "                            dereped_dir_path\n",
    "                           )\n",
    "                   )\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0fcab",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Note before next step</font>\n",
    "After dereplication, users should give a careful check of <b>size</b> of dereplicated fasta files. It is worth noting that if a fasta file with a very low amount of sequences, getting processed together with all the rest, the final \"core\" clusters will be heavily affected and may bias your analysis.\n",
    "    \n",
    "Since the core genome construction is similar with intersection construction. <font color=\"red\"><b>It is recommend to remove some very small dereplicated fasta files BEFORE NEXT STEP</b>, e.g., remove all dereplicated <i>E.coli </i> genomes with file size lower than 2.5MB as most should be larger than 4.0MB.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d271740",
   "metadata": {},
   "source": [
    "## Step 3 Sequence concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e5c34",
   "metadata": {},
   "source": [
    "This step requires **path to directory of derplicated FASTA** to generate to A FASTA file Combined from all dereplicated FASTA files made in Step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13cd34",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `dereped_dir_path` to the path to directory of dereplicated files.\n",
    "* Set variable `cated_fasta_path` as path to concatenated FASTA file.\n",
    "\n",
    "Then run the codes below [Step 3 execution](###step-3-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf284cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# directory path of derepliacted fasta directory\n",
    "dereped_dir_path = 'test_op/dereped/'\n",
    "# path save concatenated fasta\n",
    "cated_fasta_path = 'test_op/cated.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdfd75",
   "metadata": {},
   "source": [
    "### Step 3 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8372ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all fastas\n",
    "input_files = os.listdir(dereped_dir_path)\n",
    "input_files = [os.path.join(dereped_dir_path, x) for x in input_files]\n",
    "\n",
    "with open(cated_fasta_path, 'w') as outfile:\n",
    "    for fs in input_files:\n",
    "        # Open the input file in read mode\n",
    "        with open(fs, 'r') as infile:\n",
    "            \n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "            infile.close()\n",
    "            \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39aa986",
   "metadata": {},
   "source": [
    "## Step 4 Pre-clustering of concatenated FASTA and non-redundant genome generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc02cf4",
   "metadata": {},
   "source": [
    "Step 4 is the new feature in version `0.1` comparing with `0.1Beta`.It performs 100% clustering on the concatenated FASTA made in Step 3.<br>\n",
    "\n",
    "The program of Step 4 will take the **concatenated fasta made in step3 as input**, and produce:<br>\n",
    "* dereplicated concatenated FASTA\n",
    "* length of each non-redundant sequence\n",
    "* pre-clustered gene id\n",
    "* each genome that is redundancy-removed\n",
    "\n",
    "In previous versions, program would BLAST the concatenated FASTA against each dereplicated genome sequentially, as direct all-vs-all BLAST using concatenated FASTA would be too memory intensive to run (1~ G FASTA direct all-vs-all BLAST cost roughly 120 GB memory using `-mt_mode 1` with 36 threads). <br>\n",
    "\n",
    "As tested, BLAST the concatenated FASTA against each genome (`-mt_mode 1` and 36 threads) could also be very time consuming:\n",
    "* 500 <i>Listeria monocytogenes</i> costs > 7 hours.<br>\n",
    "* 1150 <i>E. coli</i> costs > 2.7 days.<br>\n",
    "\n",
    "However, when running the program for phylogneticlly close genmoes, there would be a high duplication level in the concatenated FASTA.<br>\n",
    "\n",
    "As tested, the size of concatenated FASTA could be significantly reduced after dereplication:\n",
    "* 500 <i>Listeria monocytogenes</i> before dereplication -> ~1.33GB, after -> ~187MB, \n",
    "* 1150 <i>E. coli</i> before dereplication -> ~5.2GB, after -> ~986MB\n",
    "\n",
    "Besides dereplication on concatenated FASTA, this version would also use this dereplicated concatenaion to re-generate each genome without overall redundancy (non-redundant genome). Which would significantly reduce task labor.<br>\n",
    "\n",
    "As tested, the BLAST time usage after dereplication could be significantly reduced:\n",
    "* 500 <i>Listeria monocytogenes</i> before dereplication -> ~7 hours, after -> ~22 mins\n",
    "* 1150 <i>E. coli</i> before dereplication -> ~2.7 days, after -> less than 5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54bb12",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e7f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nr_each(in_genome_lst, fasta_dict, op):\n",
    "    \n",
    "    for nr_genome in in_genome_lst:\n",
    "        saver = []\n",
    "        \n",
    "        for id_ in nr_genome: \n",
    "            saver.append(fasta_dict[id_])\n",
    "        \n",
    "        SeqIO.write(saver, \n",
    "                    os.path.join(op,\n",
    "                                 nr_genome[0][0: 5] + '.fasta'\n",
    "                                ), \n",
    "                    'fasta'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed537598",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `cated_fasta_path` as path to concatenated FASTA file made in Step 3.\n",
    "* Set variable `cated_dereped_fasta_path` as path to dereplicated concatenated FASTA file.\n",
    "* Set variable `nr_genomes_pth` as path to directory of each non-redundant genome file.\n",
    "* Set variable `len_infor_path` as path to file of sequence length information.\n",
    "* Set variable `pre_cluster_path` as path to file pre-clustered gene id.\n",
    "* Set variable `process_number` as thread number.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0397af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to concatenated fasta amde in Step 3\n",
    "cated_fasta_path = 'test_op/cated.fasta'\n",
    "# path to the dereplicated concatenated fasta\n",
    "cated_dereped_fasta_path = 'test_op/dereped_cated.fasta'\n",
    "# path to the dereplicated of non-redundant genomes\n",
    "nr_genomes_pth = 'test_op/nr_genomes'\n",
    "# path to save sequence lenght of each sequence\n",
    "len_infor_path = 'test_op/seq_len.txt'\n",
    "# path to save pre-cluster\n",
    "pre_cluster_path = 'test_op/pre_cluster.txt'\n",
    "# how many jobs to parallel\n",
    "process_number = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89cf3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cated_fasta = SeqIO.parse(cated_fasta_path,\n",
    "                           'fasta'\n",
    "                          )\n",
    "\n",
    "length_info = open(len_infor_path, 'w')\n",
    "\n",
    "pre_cluster = defaultdict(list)\n",
    "each_d = defaultdict(list)\n",
    "\n",
    "dereped_cated_dict = {}\n",
    "\n",
    "dereped_cated = []\n",
    "\n",
    "# derep of concatenated\n",
    "for rec in cated_fasta:\n",
    "    SEQ = str(rec.seq)\n",
    "    \n",
    "    if SEQ in pre_cluster.keys():# if already presence\n",
    "        pre_cluster[SEQ].append(rec.id) # must after if\n",
    "        \n",
    "    else:\n",
    "        length_info.write(rec.id \n",
    "                          + '\\t' \n",
    "                          +  str(len(SEQ)) \n",
    "                          + '\\n')\n",
    "        pre_cluster[SEQ].append(rec.id)\n",
    "        \n",
    "        dereped_cated.append(rec)\n",
    "        dereped_cated_dict[rec.id] = rec\n",
    "        each_d[rec.id[0: 5]].append(rec.id)\n",
    "        \n",
    "length_info.close()\n",
    "\n",
    "# save pre cluster\n",
    "pre_cluster_file = open(pre_cluster_path, 'w')\n",
    "\n",
    "for _100_clus in pre_cluster.values():\n",
    "    LEN = len(_100_clus)\n",
    "    LEN_1 = LEN - 1\n",
    "    for ele in range(LEN):\n",
    "        if ele == LEN_1:\n",
    "            pre_cluster_file.write(_100_clus[ele] + '\\n')\n",
    "        else:\n",
    "            pre_cluster_file.write(_100_clus[ele] + '\\t')\n",
    "\n",
    "pre_cluster_file.close()\n",
    "\n",
    "# save dereped_cated\n",
    "SeqIO.write(dereped_cated, \n",
    "            cated_dereped_fasta_path,\n",
    "            'fasta')\n",
    "\n",
    "# nr genomes\n",
    "if os.path.exists(nr_genomes_pth):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(nr_genomes_pth)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "                                       \n",
    "        \n",
    "    mission_lst = list(each_d.values())\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "        \n",
    "        p = Process(target = save_nr_each,\n",
    "                    args = (sub_mission_lst,\n",
    "                            dereped_cated_dict,\n",
    "                            nr_genomes_pth\n",
    "                           )\n",
    "                   )\n",
    "            \n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41d4f2",
   "metadata": {},
   "source": [
    "## Step 5 Reciprocal Blast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127a084",
   "metadata": {},
   "source": [
    "Step 4 will carry out the Reciprocal Blast using NCBI Blast. You can get it from [NCBI official](https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/)\n",
    " \n",
    "The pipeline will assist you to:<br>\n",
    "1. Create databases for each of all dereplicated genomes using `makeblastdb`.\n",
    "2. Using `blastn` or `blastp` to align the `dereplicated concatenated FASTA` against each of the database just made and get tabular output.\n",
    "\n",
    "### makeblastdb\n",
    "To create database to BLAST, users should provide **path to directory where all non-redundant FASTA made in Step 4 is**, and a **path to output directory where BLAST database is to store**.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6742aea",
   "metadata": {},
   "source": [
    "#### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b9513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_db(in_abs_pth, \n",
    "              db_dir_abs_pth, \n",
    "              dbtype,\n",
    "              makeblastdb_bin):\n",
    "#     if db folder exist\n",
    "    strain_naam = os.path.basename(in_abs_pth)\n",
    "    strain_naam = os.path.splitext(strain_naam)[0]\n",
    "    \n",
    "    save_dest = os.path.join(db_dir_abs_pth, strain_naam)\n",
    "    \n",
    "    if os.path.exists(save_dest):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(save_dest)\n",
    "        \n",
    "    save_naam = os.path.join(save_dest, strain_naam)\n",
    "        \n",
    "    subprocess.run([makeblastdb_bin, \n",
    "                    \"-dbtype\", dbtype, \n",
    "                    \"-in\", in_abs_pth,\n",
    "                    \"-out\", save_naam,\n",
    "                    \"-parse_seqids\"\n",
    "                   ])\n",
    "    \n",
    "def make_dbs(in_abs_pth_lst,\n",
    "             db_dir_abs_pth_,\n",
    "             dbtype_,\n",
    "             makeblastdb_bin_):\n",
    "    for s in in_abs_pth_lst:\n",
    "        make_a_db(s, \n",
    "                  db_dir_abs_pth_, \n",
    "                  dbtype_,\n",
    "                  makeblastdb_bin_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a6954",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `nr_genomes_pth` as path to directory of each non-redundant genome file.\n",
    "* Set variable `blastdb_dir_path` as path to directory to store generated databases.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `dbt` as the tpye of database to make.\n",
    "* Set variable `mbdb` to the path to makeblastdb command.\n",
    "\n",
    "Then run the codes below [makeblastdb](###makeblastdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b8236",
   "metadata": {},
   "source": [
    "In case you have installed your blast but not exported to `$PATH`, you can simply input `whereis blastn` or `whereis makeblastdb` to get the full path to your blast binary file.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80fa9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makeblastdb: /opt/conda/envs/qiime2-2022.8/bin/makeblastdb\r\n"
     ]
    }
   ],
   "source": [
    "! whereis makeblastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# directory path of output\n",
    "nr_genomes_pth = 'test_op/nr_genomes/'\n",
    "# directory path of blastdb\n",
    "blastdb_dir_path = 'test_op/dbs'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# set dbtype: nucl or prot\n",
    "dbt = 'nucl'\n",
    "# set path to makblastdb bin file\n",
    "mbdb = 'makeblastdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97c380",
   "metadata": {},
   "source": [
    "#### makeblastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940f8246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 08:31:12 2023\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10000/10000\n",
      "New DB title:  test_op/nr_genomes/10000.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10003/10003\n",
      "New DB title:  test_op/nr_genomes/10003.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10006/10006\n",
      "New DB title:  test_op/nr_genomes/10006.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10009/10009\n",
      "New DB title:  test_op/nr_genomes/10009.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10012/10012\n",
      "New DB title:  test_op/nr_genomes/10012.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10015/10015\n",
      "New DB title:  test_op/nr_genomes/10015.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10018/10018\n",
      "New DB title:  test_op/nr_genomes/10018.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10021/10021\n",
      "New DB title:  test_op/nr_genomes/10021.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10024/10024\n",
      "New DB title:  test_op/nr_genomes/10024.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10026/10026\n",
      "New DB title:  test_op/nr_genomes/10026.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10028/10028\n",
      "New DB title:  test_op/nr_genomes/10028.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10030/10030\n",
      "New DB title:  test_op/nr_genomes/10030.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10032/10032\n",
      "New DB title:  test_op/nr_genomes/10032.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10034/10034\n",
      "New DB title:  test_op/nr_genomes/10034.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10036/10036\n",
      "New DB title:  test_op/nr_genomes/10036.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10038/10038\n",
      "New DB title:  test_op/nr_genomes/10038.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 175 sequences in 0.0425789 seconds.\n",
      "Adding sequences from FASTA; added 6 sequences in 0.029633 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 9 sequences in 0.0316839 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 1508 sequences in 0.127446 seconds.\n",
      "Adding sequences from FASTA; added 1717 sequences in 0.141765 seconds.\n",
      "Adding sequences from FASTA; added 1388 sequences in 0.129418 seconds.\n",
      "Adding sequences from FASTA; added 2 sequences in 0.0361912 seconds.\n",
      "Adding sequences from FASTA; added 842 sequences in 0.095978 seconds.\n",
      "Adding sequences from FASTA; added 231 sequences in 0.0558078 seconds.\n",
      "Adding sequences from FASTA; added 160 sequences in 0.0515509 seconds.\n",
      "Adding sequences from FASTA; added 1638 sequences in 0.151338 seconds.\n",
      "Adding sequences from FASTA; added 1766 sequences in 0.158558 seconds.\n",
      "Adding sequences from FASTA; added 2955 sequences in 0.202761 seconds.\n",
      "Adding sequences from FASTA; added 1579 sequences in 0.150906 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 702 sequences in 0.10395 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 2480 sequences in 0.204591 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10027/10027\n",
      "New DB title:  test_op/nr_genomes/10027.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10025/10025\n",
      "New DB title:  test_op/nr_genomes/10025.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10029/10029\n",
      "New DB title:  test_op/nr_genomes/10029.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10004/10004\n",
      "New DB title:  test_op/nr_genomes/10004.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10007/10007\n",
      "New DB title:  test_op/nr_genomes/10007.fasta\n",
      "Sequence type: Nucleotide\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10035/10035\n",
      "New DB title:  test_op/nr_genomes/10035.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10022/10022\n",
      "New DB title:  test_op/nr_genomes/10022.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10033/10033\n",
      "New DB title:  test_op/nr_genomes/10033.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10010/10010\n",
      "New DB title:  test_op/nr_genomes/10010.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10037/10037\n",
      "New DB title:  test_op/nr_genomes/10037.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10013/10013\n",
      "New DB title:  test_op/nr_genomes/10013.fasta\n",
      "Sequence type: Nucleotide\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10016/10016\n",
      "New DB title:  test_op/nr_genomes/10016.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10019/10019\n",
      "New DB title:  test_op/nr_genomes/10019.fasta\n",
      "Sequence type: Nucleotide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10001/10001\n",
      "New DB title:  test_op/nr_genomes/10001.fasta\n",
      "Sequence type: Nucleotide\n",
      "Adding sequences from FASTA; added 9 sequences in 0.0325491 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10039/10039\n",
      "New DB title:  test_op/nr_genomes/10039.fasta\n",
      "Sequence type: Nucleotide\n",
      "Adding sequences from FASTA; added 383 sequences in 0.0555749 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 88 sequences in 0.0386209 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10031/10031\n",
      "New DB title:  test_op/nr_genomes/10031.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 124 sequences in 0.0444961 seconds.\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 1799 sequences in 0.150478 seconds.\n",
      "Adding sequences from FASTA; added 918 sequences in 0.104859 seconds.\n",
      "Adding sequences from FASTA; added 205 sequences in 0.047132 seconds.\n",
      "\n",
      "\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 1621 sequences in 0.145645 seconds.\n",
      "Adding sequences from FASTA; added 1088 sequences in 0.115187 seconds.\n",
      "Adding sequences from FASTA; added 1906 sequences in 0.161614 seconds.\n",
      "Adding sequences from FASTA; added 1521 sequences in 0.142081 seconds.\n",
      "Adding sequences from FASTA; added 1750 sequences in 0.158878 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10006/10006\n",
      "New DB title:  test_op/nr_genomes/10006.fasta\n",
      "Sequence type: Nucleotide\n",
      "Adding sequences from FASTA; added 1628 sequences in 0.156454 seconds.\n",
      "Adding sequences from FASTA; added 2946 sequences in 0.223562 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10026/10026\n",
      "New DB title:  test_op/nr_genomes/10026.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 829 sequences in 0.0991809 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 2765 sequences in 0.210656 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10020/10020\n",
      "New DB title:  test_op/nr_genomes/10020.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10023/10023\n",
      "New DB title:  test_op/nr_genomes/10023.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10017/10017\n",
      "New DB title:  test_op/nr_genomes/10017.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 182 sequences in 0.0274112 seconds.\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10008/10008\n",
      "New DB title:  test_op/nr_genomes/10008.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10014/10014\n",
      "New DB title:  test_op/nr_genomes/10014.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10011/10011\n",
      "New DB title:  test_op/nr_genomes/10011.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:12\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10012/10012\n",
      "New DB title:  test_op/nr_genomes/10012.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 293 sequences in 0.029356 seconds.\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10005/10005\n",
      "New DB title:  test_op/nr_genomes/10005.fasta\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 44 sequences in 0.020998 seconds.\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 01/18/2023 08:31:13\n",
      "New DB name:   /data/docker_qiime2_share_container_E/cpp_learn/upload_0.1/test_op/dbs/10002/10002\n",
      "New DB title:  test_op/nr_genomes/10002.fasta\n",
      "Sequence type: Nucleotide\n",
      "\n",
      "\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 1068 sequences in 0.061677 seconds.\n",
      "Adding sequences from FASTA; added 2099 sequences in 0.094882 seconds.\n",
      "\n",
      "\n",
      "Adding sequences from FASTA; added 2553 sequences in 0.111842 seconds.\n",
      "Adding sequences from FASTA; added 2558 sequences in 0.110286 seconds.\n",
      "Adding sequences from FASTA; added 1978 sequences in 0.0867701 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wed Jan 18 08:31:13 2023\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # mkdir or not\n",
    "    if os.path.exists(blastdb_dir_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(blastdb_dir_path)\n",
    "    \n",
    "    mission_lst = os.listdir(nr_genomes_pth)\n",
    "    \n",
    "    mission_lst = [os.path.join(nr_genomes_pth, x) for x in mission_lst]\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    # mp\n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "    \n",
    "        p = Process(target = make_dbs,\n",
    "                    args = (sub_mission_lst, \n",
    "                            blastdb_dir_path, \n",
    "                            dbt,\n",
    "                            mbdb\n",
    "                           )\n",
    "                   )\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3263e0",
   "metadata": {},
   "source": [
    "### Reciprocal Blast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f81848",
   "metadata": {},
   "source": [
    "To perform reciprocal BLAST, users should provide **path to dereplicated concatenated FASTA producd by step 4**, **path to directory where databases made by `makeblastdb`**, and a **path to output directory where BLAST tabular output** is to store.<br>\n",
    "\n",
    "The reason to BLAST against each database sequentially rather than directly using an all-vs-all approach is to \n",
    "reduce computational overhead. This can be very useful if the task involves many genomes. For example, if you have 1000 dereplicated genomes to analyze, the total size of concatenated FASTA may reach 5-10 GB. A multi-threaded BLAST job using the `-mt_mode 1` by all-vs-all style could be too memory-intensive to run for such a large dataset.\n",
    "\n",
    "In addition, sequentially running BLAST will produce one tabular output per database. This will be a better adaptation for the job parallelization of finding reciprocal best hits in later steps, which will apply the hash binning method.\n",
    "\n",
    "In current version `0.1`, program allow users to choose recoprocal BLAST executed under memory efficient mode or not. Under memory efficient mode, the real-time memory usage will be much lower but **much more** time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866732aa",
   "metadata": {},
   "source": [
    "#### define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e77b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_blast_low(catted_fasta_pth,\n",
    "                         db_pth, # alread abs\n",
    "                         blast_res_dir, # already abs\n",
    "                         blast_bin,\n",
    "                         e_value, \n",
    "                         t_num):\n",
    "#   get db name to blast\n",
    "    strain_naam = db_pth.split('/')[-1]\n",
    "    \n",
    "    db_name = os.path.join(db_pth, strain_naam)\n",
    "    save_dest = os.path.join(blast_res_dir, strain_naam + '.tab')\n",
    "        \n",
    "    subprocess.run([blast_bin, \n",
    "                    \"-query\", catted_fasta_pth,\n",
    "                    \"-db\", db_name,\n",
    "                    \"-out\", save_dest,\n",
    "                    \"-evalue\", str(e_value),\n",
    "                    \"-max_hsps\", \"1\",\n",
    "                    \"-dust\", \"no\",\n",
    "                    \"-outfmt\", str(\"6 qseqid sseqid score\"),\n",
    "                    \"-mt_mode\", \"1\",\n",
    "                    \"-num_threads\", str(t_num)\n",
    "                   ])\n",
    "    \n",
    "def sequential_blast_high(catted_fasta_pth,\n",
    "                          db_pth, # alread abs\n",
    "                          blast_res_dir, # already abs\n",
    "                          blast_bin,\n",
    "                          e_value):\n",
    "#   get db name to blast\n",
    "    strain_naam = db_pth.split('/')[-1]\n",
    "    \n",
    "    db_name = os.path.join(db_pth, strain_naam)\n",
    "    save_dest = os.path.join(blast_res_dir, strain_naam + '.tab')\n",
    "        \n",
    "    subprocess.run([blast_bin, \n",
    "                    \"-query\", catted_fasta_pth,\n",
    "                    \"-db\", db_name,\n",
    "                    \"-out\", save_dest,\n",
    "                    \"-evalue\", str(e_value),\n",
    "                    \"-max_hsps\", \"1\",\n",
    "                    \"-dust\", \"no\",\n",
    "                    \"-outfmt\", str(\"6 qseqid sseqid score\"),\n",
    "                    \"-mt_mode\", \"1\",\n",
    "                    \"-num_threads\", \"1\"\n",
    "                   ])\n",
    "def sequential_blast_high_s(catted_fasta_pth_,\n",
    "                            db_pth_lst, # alread abs\n",
    "                            blast_res_dir_, # already abs\n",
    "                            blast_bin_,\n",
    "                            e_value_):\n",
    "    for db in db_pth_lst:\n",
    "        sequential_blast_high(catted_fasta_pth_,\n",
    "                              db,\n",
    "                              blast_res_dir_, \n",
    "                              blast_bin_, \n",
    "                              e_value_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18271c",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `cated_dereped_fasta_path` as path to dereplicated concatenated FASTA file.\n",
    "* Set variable `blastdb_dir_path` as path to directory to store generated databases.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `E_value` as e value parameter during BLAST.\n",
    "* Set variable `blast_bin_` to the path to BLAST command.\n",
    "* Set variable `blast_op_dir` to the path of directory to save BLAST output.\n",
    "* Set variable `mem_eff` as Ture or False to run under memory efficient mode or not.\n",
    "\n",
    "Then run the codes below [BLAST](###BLAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c643bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blastn: /opt/conda/envs/qiime2-2022.8/bin/blastn\r\n"
     ]
    }
   ],
   "source": [
    "! whereis blastn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1456d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# path to concatenated fasta\n",
    "cated_dereped_fasta_path = 'test_op/dereped_cated.fasta'\n",
    "# directory path of blastdb\n",
    "blastdb_dir_path = 'test_op/dbs/'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# set e_value\n",
    "E_value = 1e-5\n",
    "# set path to blast bin file\n",
    "blast_bin_ = 'blastn'\n",
    "# set blast output directory\n",
    "blast_op_dir = 'test_op/blast_op'\n",
    "# memory efficient mode \n",
    "mem_eff_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7aba86",
   "metadata": {},
   "source": [
    "#### BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d2d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 19 15:11:31 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Process Process-8:\n",
      "Process Process-13:\n",
      "Process Process-16:\n",
      "Process Process-12:\n",
      "Process Process-9:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "Process Process-2:\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "Process Process-11:\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "Process Process-7:\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "Process Process-14:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m             jobs\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m jobs:\n\u001b[0;32m---> 47\u001b[0m             \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39masctime())\n",
      "File \u001b[0;32m/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 53, in sequential_blast_high_s\n",
      "    sequential_blast_high(catted_fasta_pth_,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/tmp/ipykernel_3306/3804186049.py\", line 36, in sequential_blast_high\n",
      "    subprocess.run([blast_bin,\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 495, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1020, in communicate\n",
      "    self.wait()\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "  File \"/opt/conda/envs/qiime2-2022.8/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())\n",
    "# mkdir or not\n",
    "\n",
    "if os.path.exists(blast_op_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(blast_op_dir)\n",
    "    \n",
    "    \n",
    "mission_lst = os.listdir(blastdb_dir_path)\n",
    "mission_lst = [os.path.join(blastdb_dir_path, x) for x in mission_lst]\n",
    "\n",
    "if mem_eff_mode:\n",
    "    for db in mission_lst:\n",
    "        sequential_blast_low(cated_dereped_fasta_path, \n",
    "                         db, \n",
    "                         blast_op_dir, \n",
    "                         blast_bin_, \n",
    "                         E_value, \n",
    "                         process_number)\n",
    "        \n",
    "else:\n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        # mp\n",
    "        jobs = []\n",
    "\n",
    "        for db_lst in mission_lst:\n",
    "\n",
    "            p = Process(target = sequential_blast_high_s,\n",
    "                        args = (cated_dereped_fasta_path, \n",
    "                                db_lst, \n",
    "                                blast_op_dir,\n",
    "                                blast_bin_,\n",
    "                                E_value\n",
    "                               )\n",
    "                       )\n",
    "            p.start()\n",
    "            jobs.append(p)\n",
    "\n",
    "\n",
    "        for z in jobs:\n",
    "            z.join()\n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aadde9",
   "metadata": {},
   "source": [
    "## Step 6 query binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06625672",
   "metadata": {},
   "source": [
    "This is the new feature in version `0.1`. This step is to apply hash binning to bin all presence of a query into same file to facilitate next step filtering.\n",
    "\n",
    "<font color=\"red\"><b>Set bin level:</b></font><br>\n",
    "According to the amount of genomes to analysze, user should provide binning level, which is to set how many bins should be used. Level $L$ should be interger within range $0 < L \\le 9999$, and will generate $L$ bins. \n",
    "\n",
    "Suggestion is that do not set the bin level too high, especially when less than 200 genomes participated. If such amount of genomes participated analysis, bin level from 10 to 100 should work as most efficient way. \n",
    "\n",
    "As tested, an analysis of 30 genomes, has 30 BLAST output after step 5. \n",
    "* A bin level of 10, takes 7 seconds to finish, \n",
    "* a bin level of 100, takes 10 seconds to finish,\n",
    "* a bin level of 1000, takes 24 seconds to finish,\n",
    "\n",
    "<font color=\"red\"><b>When to set a high bin level:</b></font><br>\n",
    "Simply speaking, when you have really larger amount of genomes and not enough memory (e.g., more than 1000 genomes and less than 100 GB memory) <br>\n",
    "\n",
    "The output of BLAST for 1000 genomes can reach 150 GB in size, and if the bin level is set to 10, there will be 10 bins to evenly distribute the data. On average, each bin will contain 1.5 GB of data, which may be too memory-intensive to process in step 6 (where requires approximately 1.5 GB of memory per bin). However, if the number of bins is increased to 1000, the size of each bin will be reduced to between 100-200 MB, which then facilitate step 7 parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf897311",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "235f0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_binning(in_f_lst, bin_level, bin_op_dir, no_lock_mode, LOCKS):\n",
    "    for in_f in in_f_lst:\n",
    "        f = open(in_f, 'r')\n",
    "\n",
    "        save_dict = defaultdict(list)\n",
    "\n",
    "        while True:\n",
    "            l_ = f.readline()\n",
    "            if l_ == '':\n",
    "                break\n",
    "\n",
    "            query = l_.split('\\t')[0]\n",
    "\n",
    "            bin_ = hash(query) % bin_level\n",
    "\n",
    "            save_dict[bin_].append(l_)\n",
    "        f.close()\n",
    "        \n",
    "        # saving\n",
    "        for b in save_dict.keys():\n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].acquire()\n",
    "\n",
    "            f = open(os.path.join(bin_op_dir, str(b) + '.txt'), 'a')\n",
    "\n",
    "            f.writelines(save_dict[b])\n",
    "\n",
    "            f.close()\n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533201e2",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `blast_op_dir` to the path of directory to save BLAST output.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `b_level` as how many bins to generate.\n",
    "* Set len_infor_path `query_bin_op_dir_` to the path of output directory.\n",
    "\n",
    "Then run the codes below [Step 6 execution](###Step-6-exeution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb5dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# set blast output directory\n",
    "blast_op_dir = 'test_op/blast_op'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# set bin level\n",
    "b_level = 64\n",
    "# set blast output directory\n",
    "query_bin_op_dir_ = 'test_op/query_binning'\n",
    "# set whether turn off lock\n",
    "no_lock_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae5996",
   "metadata": {},
   "source": [
    "### Step 6 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "566de097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 08:33:11 2023\n",
      "Wed Jan 18 08:33:13 2023\n"
     ]
    }
   ],
   "source": [
    "if b_level < 0 or b_level > 9999 or not isinstance(b_level, int):\n",
    "    print('b_level must be integer within range 0 < L <= 9999')\n",
    "    sys.exit()\n",
    "    \n",
    "# mkdir or not\n",
    "\n",
    "if os.path.exists(query_bin_op_dir_):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(query_bin_op_dir_)\n",
    "    \n",
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mission_lst = os.listdir(blast_op_dir)\n",
    "    \n",
    "    mission_lst = [os.path.join(blast_op_dir, x) for x in mission_lst]\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    # mp\n",
    "    locks = [Lock() for i in range(b_level)]\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "    \n",
    "        p = Process(target = q_binning,\n",
    "                    args = (sub_mission_lst, \n",
    "                            b_level, \n",
    "                            query_bin_op_dir_,\n",
    "                            no_lock_mode,\n",
    "                            locks\n",
    "                           )\n",
    "                   )\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407698f",
   "metadata": {},
   "source": [
    "## Step 7 Filtering and binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b83db5",
   "metadata": {},
   "source": [
    "This step is to filter the blast output and to apply hash binning, in order to provide best preparation for reciprocal best find.<br>\n",
    "\n",
    "Step 7 requires **path to directory of query binning output**, **sequence length information, pre-cluster information output by Step 4** as input.\n",
    "\n",
    "The pipeline will carry out following treatment to BLAST output:\n",
    "1. Paralog removal: <br>\n",
    "If query and subject is from same strain, the hit will be skipped, as to remove paralog.\n",
    "2. Length ratio filtering:<br>\n",
    "Within a hit, query length $Q$ and subject length $S$, the ratio $v$ of this 2 length\n",
    "\n",
    "$$v = \\frac{Q}{S}$$\n",
    "\n",
    "should be within a range $r$, according to [L. Salichos et al](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018755), $r$ is recommended to be higher than 0.3 which means the shorter sequence should not be shorter than 30% of the longer sequence:\n",
    "\n",
    "$$r < v < \\frac{1}{r}$$<br>\n",
    "\n",
    "If above condition not met, the hit will be removed from analysis.\n",
    "\n",
    "3. Non-best-hit removal: <br>\n",
    "* Identical sequences are always regarded as best hit.\n",
    "* If a query has more than 1 subject hits, only the query-subject pair with highest score will then be kept.\n",
    "* if pairs are of same score, the pair whose query and subject are of more similar length will be kept.\n",
    "4. Sorting and binning:<br>\n",
    "For every kept hit, its query and subject will be sorted using Python or C++ built in sort algorithm. This is because in a sequential blast output file, only \"<b>single direction best hit</b>\" can be obtained, its \"<b>reciprocal best hit</b>\" only exist in other files, which poses difficulty doing \"<b>repriprocal finding</b>\". <br>\n",
    "However, if a query $a$ and its best suject hit $b$, passed filter above, and form $(a, b)$, and in the mean time we sort its rericprocal hit $(b, a)$ from another file into $(a, b)$, then both $(a, b)$ will generate same hash value. This hashed value with last several digits will allow us to bin them into same new file. Therefore, after this binning, \"<b>reciprocal finding</b>\" will be turned into \"<b>duplication finding</b>\" within one same file.<br>\n",
    "\n",
    "<font color=\"red\"><b>Set bin level:</b></font><br>\n",
    "According to the amount of genomes to analysze, user should provide binning level, which is to set how many bins should be used. Level $L$ should be interger within range $0 < L \\le 9999$, and will generate $L$ bins. \n",
    "\n",
    "Suggestion is that do not set the bin level too high, especially when less than 200 genomes participated. If such amount of genomes participated analysis, bin level from 10 to 100 should work as most efficient way. \n",
    "\n",
    "<font color=\"red\"><b>When to set a high bin level:</b></font><br>\n",
    "Simply speaking, when you have really larger amount of genomes and not enough memory (e.g., more than 1000 genomes and less than 100 GB memory) <br>\n",
    "\n",
    "The output of BLAST for 1000 genomes can reach 200 GB in size, and if the bin level is set to 2, there will be 100 bins to evenly distribute the data. On average, each bin will contain 1.7 GB of data, which may be too memory-intensive to process in step 7, where reciprocal find is performed (which requires approximately 1.7 GB of memory per bin). However, if the number of bins is increased to 1000, the size of each bin will be reduced to between 100-200 MB, which then facilitate step 7 parallelization.\n",
    "\n",
    "<font color=\"red\"><b>Note:</b></font><br>\n",
    "This is the one of the most computation and I/O intensive step, use the C++ based binary file to process for better efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72027951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileter_n_bin(blast_op_pt_lst, # to avoid redundant copy of  seqlen info, abs pth\n",
    "                  r, \n",
    "                  bin_level, \n",
    "                  bin_op_dir, # already abs\n",
    "                  seq_len_info,\n",
    "                  bin_spe,\n",
    "                  no_lock_mode,\n",
    "                  LOCKS):\n",
    "    for fs in blast_op_pt_lst:\n",
    "        \n",
    "        f = open(fs, 'r')\n",
    "            \n",
    "        q_s_d = defaultdict(dict)\n",
    "        \n",
    "        sspe_m = defaultdict(lambda: defaultdict(list\n",
    "                                                )\n",
    "                            )\n",
    "        \n",
    "        while True:\n",
    "            l_ = f.readline()\n",
    "            if l_ == '':\n",
    "                break\n",
    "                \n",
    "#             l_ = l_.rstrip('\\n') '\\n' do not affect number  \n",
    "            l_ = l_.split('\\t')\n",
    "            \n",
    "            query = l_[0]\n",
    "            subject = l_[1]\n",
    "            \n",
    "            if query == subject: # if self hit\n",
    "                continue\n",
    "            else:\n",
    "                score = int(l_[2])\n",
    "                \n",
    "                q_s_d[query][subject] = score\n",
    "                \n",
    "                # add all spe\n",
    "                for a_spe_of_clus in bin_spe[subject]:\n",
    "                    sspe_m[query][a_spe_of_clus].append(subject)\n",
    "#                     {\n",
    "#                         q1: {\n",
    "#                             a1_spe_in_sub_cluster: [subject1 id,\n",
    "#                                                     subject2 id\n",
    "#                                                    ],\n",
    "#                             a2_spe_in_sub_cluster: [subject3 id,\n",
    "#                                                     subject4 id\n",
    "#                                                    ],\n",
    "#                         },\n",
    "#                         q2\n",
    "#                     }\n",
    "                \n",
    "        # how to save\n",
    "        saver = defaultdict(list)\n",
    "        \n",
    "        for qs in q_s_d.keys():\n",
    "            q_len = seq_len_info.loc[qs, 1]\n",
    "            \n",
    "            for sub in q_s_d[qs].keys():\n",
    "                s_spe = sub[0: 5]\n",
    "                \n",
    "                if qs[0: 5] == s_spe:# if paralog\n",
    "                    continue\n",
    "                elif s_spe in bin_spe[qs]:# if qs cluster has that spe\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    best_hit = True\n",
    "                    \n",
    "                    same_score = []\n",
    "                    \n",
    "                    if len(sspe_m[qs][s_spe]) > 1: #if multiple hit\n",
    "                        for id_ in sspe_m[qs][s_spe]:\n",
    "                            if sub == id_: # skip self\n",
    "                                continue\n",
    "                                \n",
    "                            if q_s_d[qs][sub] < q_s_d[qs][id_]: # if there is a better hit\n",
    "                                best_hit = False\n",
    "                                break\n",
    "                            elif q_s_d[qs][sub] == q_s_d[qs][id_]: # same score \n",
    "                                same_score.append(id_)\n",
    "                    \n",
    "                    if len(same_score) > 0 and best_hit: # need more similar length # print(qs + \"<>\" + sub)\n",
    "                        \n",
    "                        qs_len_diff = abs(q_len - seq_len_info.loc[sub, 1])\n",
    "                        \n",
    "                        for id_ in same_score:\n",
    "                            q_id_len_diff = abs(q_len - seq_len_info.loc[id_, 1])\n",
    "                            \n",
    "                            if q_id_len_diff < qs_len_diff:\n",
    "#                                 print(qs + \"< >\" + sub + \"< >\" + id_)\n",
    "                                best_hit = False\n",
    "                                break\n",
    "                                \n",
    "                    if best_hit: \n",
    "                        \n",
    "                        len_ratio = q_len/seq_len_info.loc[sub, 1]\n",
    "                        \n",
    "                        c1 = len_ratio < r\n",
    "                        c2 = len_ratio > (1/r)\n",
    "                        \n",
    "                        if c1 or c2: # len ratio filter\n",
    "#                             print(qs + \"< >\" + sub)\n",
    "                            continue\n",
    "                        else:\n",
    "                            sorting = sorted((qs, sub)) \n",
    "                            \n",
    "                            bin_ = hash(tuple(sorting\n",
    "                                             )\n",
    "                                       ) % bin_level\n",
    "                            \n",
    "                            saver[str(bin_)].append(sorting[0] + '\\t' + sorting[1] + '\\n')\n",
    "                            \n",
    "        for b in saver.keys():\n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].acquire()\n",
    "\n",
    "            f = open(os.path.join(bin_op_dir, b + '.txt'), 'a')\n",
    "\n",
    "            f.writelines(saver[b])\n",
    "\n",
    "            f.close()\n",
    "            \n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0379bb",
   "metadata": {},
   "source": [
    "### Set path\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `b_level` as how many bins to generate.\n",
    "* Set variable `r_` as the limit of length difference ratio.\n",
    "* Set variable `blast_op_dir` to the path of directory to save BLAST output.\n",
    "* Set variable `len_infor_path` as path to file of sequence length information.\n",
    "* Set variable `pre_cluster_path` as path to file pre-clustered gene id.\n",
    "\n",
    "Then run the codes below [Step 6 execution](###Step-6-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d251d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own\n",
    "# set blast output directory\n",
    "qbin_op_dir = 'test_op/query_binning/'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# set bin level\n",
    "b_level = 32\n",
    "# set length ration limit r value \n",
    "r_ = 0.5\n",
    "# set blast output directory\n",
    "bin_op_dir_ = 'test_op/bin_op'\n",
    "# path save sequence lenght of each sequence\n",
    "len_infor_path = 'test_op/seq_len.txt'\n",
    "# path to save pre-cluster\n",
    "pre_cluster_path = 'test_op/pre_cluster.txt'\n",
    "# set whether turn off lock\n",
    "no_lock_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b682c",
   "metadata": {},
   "source": [
    "### Step 6 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce5220dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 20 09:23:30 2023\n",
      "Fri Jan 20 09:23:35 2023\n"
     ]
    }
   ],
   "source": [
    "if b_level < 0 or b_level > 9999 or not isinstance(b_level, int):\n",
    "    print('b_level must be integer within range 0 < L <= 9999')\n",
    "    sys.exit()\n",
    "\n",
    "# mkdir or not\n",
    "\n",
    "if os.path.exists(bin_op_dir_):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(bin_op_dir_)\n",
    "\n",
    "# seq len\n",
    "seq_len_df = pd.read_csv(len_infor_path, \n",
    "                         sep='\\t',\n",
    "                         header = None,\n",
    "                         dtype = {\n",
    "                             0: 'str',\n",
    "                             1: 'int'\n",
    "                         }\n",
    "                        ).set_index([0])\n",
    "\n",
    "# bin spe\n",
    "bin_spe = {}\n",
    "\n",
    "f = open(pre_cluster_path, 'r')\n",
    "\n",
    "while True:\n",
    "    l_ = f.readline()\n",
    "    if l_ == '':\n",
    "        break\n",
    "    \n",
    "    \n",
    "    l_ = l_.rstrip('\\n').split('\\t')\n",
    "    \n",
    "    bin_ = l_[0]\n",
    "    \n",
    "    bin_spe[bin_] = [x[0: 5] for x in l_]\n",
    "    \n",
    "\n",
    "f.close()\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mission_lst = os.listdir(qbin_op_dir)\n",
    "    \n",
    "    mission_lst = [os.path.join(qbin_op_dir, x) for x in mission_lst]\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    # mp\n",
    "    locks = [Lock() for i in range(b_level)]\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "    \n",
    "        p = Process(target = fileter_n_bin,\n",
    "                    args = (sub_mission_lst, \n",
    "                            r_, \n",
    "                            b_level,\n",
    "                            bin_op_dir_,\n",
    "                            seq_len_df,\n",
    "                            bin_spe,\n",
    "                            no_lock_mode,\n",
    "                            locks\n",
    "                           )\n",
    "                   )\n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d89fe",
   "metadata": {},
   "source": [
    "## Step 8 Reciprocal Best find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e6332",
   "metadata": {},
   "source": [
    "This Step is to find reciprocal best hits. In `Step 7`, query-subject pairs had been binned into different files according to their hash value, therefore, pair $(a, b)$ and its reciprocal pair $(b, a)$ (which was sorted into $(a, b)$), will be in the same bin. Thus, a pair found twice in a bin will be reported as a reciprocal best blast pair.\n",
    "\n",
    "In addition, Step 8 also does hash binning after a reciprocal best hit is comfirmed. Query-subject pairs will be binned by the hash value of query ID, which then put pairs with common elements into same bin to assist faster clustering in next step.\n",
    "\n",
    "Step 8 requires **path to directory of bins output by Step 7**, and path to output directory.\n",
    "\n",
    "\n",
    "<font color=\"red\"><b>Set bin level:</b></font><br>\n",
    "According to the amount of genomes to analysze, user should provide binning level, which is to set how many bins should be used. Level $0 < L \\le 9999$, and will generate $L$ bins. \n",
    "\n",
    "Suggestion is that do not set the bin level too high, especially when less than 200 genomes participated. If such amount of genomes participated analysis, bin level from 10 to 100 should work as most efficient way. \n",
    "\n",
    "As tested, 30 genomes, if 10 bins generated by Step 7: \n",
    "* A bin level of 10, takes 1 seconds to finish, \n",
    "* a bin level of 100, takes 2 seconds to finish,\n",
    "* a bin level of 1000, takes 7 seconds to finish,\n",
    "\n",
    "<font color=\"red\"><b>When to set a high bin level:</b></font><br>\n",
    "Simply speaking, when you have really larger amount of genomes and not enough memory (e.g., more than 1000 genomes and less than 100 GB memory) <br>\n",
    "\n",
    "Less bins could make step 8 faster, but step 9 more memory intensive.\n",
    "\n",
    "<font color=\"red\"><b>Note:</b></font><br>\n",
    "This is the one of the most computation and I/O intensive step, use the C++ based binary file to process for better efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f993312",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3684157",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dup_find_mem(in_file_lst, RBB_op_dir, bin_level, no_lock_mode, LOCKS):\n",
    "    for in_file in in_file_lst:\n",
    "        naam = os.path.basename(in_file)\n",
    "\n",
    "        f = open(in_file, 'r')\n",
    "\n",
    "        rec = set()\n",
    "        d_bin = defaultdict(list)\n",
    "\n",
    "        while True:\n",
    "            l_ = f.readline()\n",
    "            if l_ == '':\n",
    "                break\n",
    "\n",
    "            if l_ in rec:\n",
    "                hash_res = str(hash(l_.split('\\t')[0]\n",
    "                                   ) % bin_level\n",
    "                              )\n",
    "                d_bin[hash_res].append(l_)\n",
    "                rec.remove(l_)\n",
    "            else:\n",
    "                rec |= set([l_])\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        for b in d_bin.keys():\n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].acquire()\n",
    "\n",
    "            f2 = open(os.path.join(RBB_op_dir, b + '.txt'), 'a')\n",
    "\n",
    "            f2.writelines(d_bin[b])\n",
    "\n",
    "            f2.close()\n",
    "            if not no_lock_mode:\n",
    "                LOCKS[int(b)].release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b583b2",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `bin_op_dir_` as path to directory to bins generated by Step 6.\n",
    "* Set variable `RBB_op_dir_pth` as path to output directory.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `b_level` as how many bins to generate\n",
    "\n",
    "Then run the codes below [Step 7 execution](###Step-7-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "706fe1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set blast output directory\n",
    "bin_op_dir_ = 'test_op/bin_op/'\n",
    "# set reciprocal best output directory\n",
    "RBB_op_dir_pth = 'test_op/RBB_op'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# set bin level\n",
    "b_level = 1000\n",
    "# set whether turn off lock\n",
    "no_lock_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cf269",
   "metadata": {},
   "source": [
    "### Step 7 execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "472d946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 08:55:23 2023\n",
      "Wed Jan 18 08:55:40 2023\n"
     ]
    }
   ],
   "source": [
    "if b_level < 0 or b_level > 9999 or not isinstance(b_level, int):\n",
    "    print('b_level must be integer within range 0 < L <= 9999')\n",
    "    sys.exit()\n",
    "\n",
    "# mkdir or not\n",
    "if os.path.exists(RBB_op_dir_pth):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(RBB_op_dir_pth)\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mission_lst = os.listdir(bin_op_dir_)\n",
    "    \n",
    "    mission_lst = [os.path.join(bin_op_dir_, x) for x in mission_lst]\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    # mp\n",
    "    locks = [Lock() for i in range(b_level)\n",
    "            ]\n",
    "             \n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "        \n",
    "        p = Process(target = dup_find_mem,\n",
    "                    args = (sub_mission_lst,\n",
    "                            RBB_op_dir_pth,\n",
    "                            b_level,\n",
    "                            no_lock_mode,\n",
    "                            locks\n",
    "                           )\n",
    "                   )\n",
    "            \n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7800e9",
   "metadata": {},
   "source": [
    "## Step 9 Single Linkage Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d738370",
   "metadata": {},
   "source": [
    "This step will carry out single linkage clustering on output from step 8. Users may perform \"<b>multi-step-to-final</b>\" or \"<b>one-step-to-final</b>\" clustering by adjusting the `compression_size` parameter. In the output files, each row is a cluster (stopped by \"\\n\") and each gene ID is separated by \"\\t\".\n",
    "\n",
    "In case that large amount genomes participated analysis, it could be memory intensive to reach final cluster in a single step. The pipeline provide ability to extenuate such pressure by reaching final cluster with multiple steps. For example, if `compression_size` = 5 is provided, program will perform clustering using 5 files at a time and shrink the output file number by a factor of 5.\n",
    "\n",
    "<font color=\"red\"><b>Note Before Start</b></font><br>\n",
    "User <font color=\"red\"><b>must</b></font> specify the path to `pre-cluster file` produced in `Step 4`, when running the <font color=\"red\"><b>LAST step of multi step to final</b></font>, or when running direct <font color=\"red\"><b>one step to final</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e5b87",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c44acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_bin(bin_ind_lst, ind):\n",
    "    while bin_ind_lst[ind] != ind:\n",
    "        ind = bin_ind_lst[ind]\n",
    "    return ind\n",
    "\n",
    "def ptr_like_merger(in_p_lst, op_dir):\n",
    "    \n",
    "    ptr_coll = dict()\n",
    "    bin_inds_connector = []\n",
    "    \n",
    "    to_save_lst = []\n",
    "    \n",
    "    const_ind_every = 0\n",
    "    \n",
    "    for in_p in in_p_lst[1]:\n",
    "        f = open(in_p, 'r')\n",
    "    \n",
    "        while True:\n",
    "            l_ = f.readline()\n",
    "\n",
    "            if l_ == '':\n",
    "                break\n",
    "\n",
    "            l_ = set(l_.rstrip('\\n').split('\\t'))\n",
    "\n",
    "            ind_to_go = const_ind_every\n",
    "            to_save_lst.append(l_)\n",
    "            bin_inds_connector.append(ind_to_go)\n",
    "\n",
    "            for ele in l_:\n",
    "                if ele not in ptr_coll.keys():\n",
    "                    ptr_coll[ele] = ind_to_go\n",
    "\n",
    "                else:\n",
    "                    its_lowest_bin_ind = connect_bin(bin_inds_connector, \n",
    "                                                     ptr_coll[ele]\n",
    "                                                    )\n",
    "\n",
    "                    if its_lowest_bin_ind == ind_to_go:\n",
    "                        continue\n",
    "\n",
    "                    if its_lowest_bin_ind > ind_to_go:\n",
    "                        its_lowest_bin_ind, ind_to_go = ind_to_go, its_lowest_bin_ind\n",
    "\n",
    "                    to_save_lst[its_lowest_bin_ind] |= to_save_lst[ind_to_go]\n",
    "                    to_save_lst[ind_to_go] = None\n",
    "\n",
    "                    bin_inds_connector[ind_to_go] = its_lowest_bin_ind\n",
    "                    ind_to_go = its_lowest_bin_ind\n",
    "\n",
    "\n",
    "            const_ind_every += 1\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "    f2 = open(os.path.join(op_dir, str(in_p_lst[0]) + '.txt'), 'w')\n",
    "    for i in to_save_lst:\n",
    "        if i:\n",
    "            i = tuple(i)\n",
    "            Len = len(i)\n",
    "            for j in list(range(Len)):\n",
    "                if j == Len - 1: \n",
    "                    f2.write(i[j] + '\\n')\n",
    "                else:\n",
    "                    f2.write(i[j] + '\\t')\n",
    "        else:\n",
    "            pass\n",
    "    f2.close()\n",
    "                \n",
    "def files_clustering_s(in_p_lst_lst, op_dir_):\n",
    "    \n",
    "    for inp_lst in in_p_lst_lst:\n",
    "        ptr_like_merger(inp_lst, op_dir_)\n",
    "        \n",
    "def SLC(ip, op, proc_num, c_s, pre_clus_pth = ''):\n",
    "    if os.path.exists(op):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(op)\n",
    "\n",
    "    print(time.asctime())\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        mission_lst = os.listdir(ip)\n",
    "\n",
    "        if c_s > len(mission_lst):\n",
    "            print('Error: Compression size can only be smaller than ' + str(len(mission_lst)))\n",
    "        else:\n",
    "\n",
    "            mission_lst = [os.path.join(ip, x) for x in mission_lst]\n",
    "            \n",
    "            if pre_clus_pth != '':\n",
    "                mission_lst.append(pre_clus_pth)\n",
    "            \n",
    "            per = len(mission_lst)//c_s\n",
    "\n",
    "            mission_lst = list(np.array_split(mission_lst, \n",
    "                                              per\n",
    "                                             )\n",
    "                               )\n",
    "\n",
    "            x = 0\n",
    "            mission_lst_ = []\n",
    "            for i in mission_lst:\n",
    "                mission_lst_.append([x, i])\n",
    "                x += 1\n",
    "\n",
    "            if len(mission_lst_) < proc_num:\n",
    "                proc_num = len(mission_lst_)\n",
    "\n",
    "\n",
    "            mission_lst_ = list(np.array_split(mission_lst_, \n",
    "                                               proc_num\n",
    "                                              )\n",
    "                               )\n",
    "            # mp\n",
    "            jobs = []\n",
    "            \n",
    "\n",
    "            for sub_mission_lst in mission_lst_:\n",
    "\n",
    "                p = Process(target = files_clustering_s,\n",
    "                            args = (sub_mission_lst,\n",
    "                                    op\n",
    "                                   )\n",
    "                           )\n",
    "                p.start()\n",
    "                jobs.append(p)\n",
    "\n",
    "\n",
    "            for z in jobs:\n",
    "                z.join()\n",
    "\n",
    "        print(time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acc2a9",
   "metadata": {},
   "source": [
    "### Example to reach final cluster by multi step, starting with 1000 input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9782ded",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `RBB_op_dir_pth` as path to output directory.\n",
    "* Set variable `SLC_inter_op_dir_pth` as path to output directory.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `compression_size` as how many files to cluster into one file.\n",
    "* Set variable `pre_cluster_path` as path to file pre-clustered gene id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffeb35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set reciprocal best output directory\n",
    "RBB_op_dir_pth = 'test_op/RBB_op/'\n",
    "# set path to directory of clustering output\n",
    "SLC_inter_op_dir_pth = 'test_op/SLC_1/'\n",
    "# how many jobs to parallel\n",
    "process_number = 36\n",
    "# Compression size\n",
    "compression_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42455e3c",
   "metadata": {},
   "source": [
    "Then run the codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f381a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 09:00:34 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qiime2-2022.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 09:00:35 2023\n"
     ]
    }
   ],
   "source": [
    "SLC(RBB_op_dir_pth, SLC_inter_op_dir_pth, process_number, compression_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dedd17",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `ip_dir_pth` as path to output directory of last round clustering.\n",
    "* Set variable `SLC_inter_op_dir_pth` as path to output directory.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `compression_size` as how many files to cluster into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "932f7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to last step clustering output directory\n",
    "ip_dir_pth = 'test_op/SLC_1/'\n",
    "# set path to directory of clustering output\n",
    "SLC_inter_op_dir_pth = 'test_op/SLC_2/'\n",
    "# how many jobs to parallel\n",
    "process_number = 36\n",
    "# Compression size\n",
    "compression_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff543b",
   "metadata": {},
   "source": [
    "Then run the codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40f032b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 09:00:51 2023\n",
      "Wed Jan 18 09:00:52 2023\n"
     ]
    }
   ],
   "source": [
    "SLC(ip_dir_pth, SLC_inter_op_dir_pth, process_number, compression_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9f47f",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `ip_dir_pth` as path to output directory of last round clustering.\n",
    "* Set variable `SLC_inter_op_dir_pth` as path to output directory.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `compression_size` as how many files to cluster into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff1d2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to last step clustering output directory\n",
    "ip_dir_pth = 'test_op/SLC_2/'\n",
    "# set path to directory of each clustering output\n",
    "SLC_inter_op_dir_pth = 'test_op/SLC_final/'\n",
    "# how many jobs to parallel\n",
    "process_number = 10\n",
    "# Compression size\n",
    "compression_size = len(os.listdir(ip_dir_pth))\n",
    "# path to pre-cluster\n",
    "pre_cluster_path = 'test_op/pre_cluster.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3d822",
   "metadata": {},
   "source": [
    "Then run the codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41851593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 18 09:01:33 2023\n",
      "Wed Jan 18 09:01:34 2023\n"
     ]
    }
   ],
   "source": [
    "SLC(ip_dir_pth, SLC_inter_op_dir_pth, process_number, compression_size, pre_cluster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02193",
   "metadata": {},
   "source": [
    "## Step 10 Write clusters into FASTA\n",
    "In Step 9, program will help user to generate FASTA file for each cluster. By providing the final one cluster file generated by Step 8 as input, program produces 3 types of clusters into 3 directories separately.<br>\n",
    "\n",
    "Noteably, those genomes \n",
    "* depreplicated in **Step 2**,  \n",
    "* not removed because of too low genome size\n",
    "* participated processes up to this step, \n",
    "\n",
    "are used to separate 3 types of clusters.<br>\n",
    "\n",
    "1. In drectory `accessory_cluster`(a cluster not shared by all genomes), FASTA files of clusters, which **do not have genes from all genomes** participated analysis, will be output in this drectory. For example, there are 100 genomes in analysis, a cluster with less than 100 genes will have its FASTA output here. Also, if a cluster has >= 100 genes, but all these genes are from less than 100 genomes, its FASTA will be in this directory.\n",
    "2. In drectory `strict_core`, each cluster has **exactly 1 gene from every genome** to analyze. Such clusters will have their FASTA files here.\n",
    "3. In drectory `surplus_core`, each cluster has **at least 1 gene from every genome** to analyze, and **some genomes has more than 1 genes** in this cluster. Such clusters will have their FASTA files here.\n",
    "\n",
    "This step also requires the concatenated FASTA made in Step 3 (the one NOT dereplicated by Step 4) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1789811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_a_cluster(a_cluster_of_genes, \n",
    "                    total_amount, \n",
    "                    which_to_write, \n",
    "                    op_path,\n",
    "                    fasta_dict,\n",
    "                    ):\n",
    "    spe_count = len(set([x[0: 5] for x in a_cluster_of_genes]))\n",
    "    \n",
    "    if spe_count < total_amount:\n",
    "        if which_to_write[\"accessory_cluster\"]:\n",
    "            file_naam = os.path.join(op_path, 'accessory_cluster/' \n",
    "                                     + a_cluster_of_genes[0] \n",
    "                                     + '.fasta'\n",
    "                                    )\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        if len(a_cluster_of_genes) == total_amount:\n",
    "            if which_to_write[\"strict_core\"]:\n",
    "                file_naam = os.path.join(op_path, 'strict_core/' \n",
    "                                         + a_cluster_of_genes[0] \n",
    "                                         + '.fasta'\n",
    "                                        )\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            if which_to_write[\"surplus_core\"]:\n",
    "                file_naam = os.path.join(op_path, 'surplus_core/' \n",
    "                                         + a_cluster_of_genes[0] \n",
    "                                         + '.fasta'\n",
    "                                        )\n",
    "            else:\n",
    "                return\n",
    "    \n",
    "    to_save = []\n",
    "    for  recs in a_cluster_of_genes:\n",
    "        to_save.append(fasta_dict[recs])\n",
    "        \n",
    "    SeqIO.write(to_save, \n",
    "                file_naam, \n",
    "                'fasta')\n",
    "\n",
    "def write_cluster_s(lst_of_a_clusters, \n",
    "                    total_amount_, \n",
    "                    which_to_write_, \n",
    "                    op_path_, \n",
    "                    fasta_dict_):\n",
    "    \n",
    "    for a_cl in lst_of_a_clusters:\n",
    "        write_a_cluster(a_cl, \n",
    "                        total_amount_,\n",
    "                        which_to_write_,\n",
    "                        op_path_,\n",
    "                        fasta_dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4959d6e",
   "metadata": {},
   "source": [
    "#### Set path\n",
    "* Set variable `final_cluster_path` as path to the final cluster file made by Step 8.\n",
    "* Set variable `output_path` as path to output directory.\n",
    "* Set variable `process_number` thread number.\n",
    "* Set variable `cluster_type` select from `accessory/strict/surplus`, separate by comma (`,`).\n",
    "* Set variable `cated_fasta_path` as path to dereplicated concatenated FASTA file made in Step 4.\n",
    "* Set variable `pre_cluster_path` as path to file pre-clustered gene id made in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c3da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the final cluster file made by Step 7\n",
    "final_cluster_path = 'test_op/SLC_final/0.txt'\n",
    "# set path to output directory\n",
    "output_path = 'test_op/write_fasta_py'\n",
    "# how many jobs to parallel\n",
    "process_number = 16\n",
    "# select at least 1 from `accessory/strict/surplus`, separate by comma (`,`), \n",
    "# specifying types of clusters to write\n",
    "cluster_type = 'accessory,strict,surplus'\n",
    "# set path to concatenated FASTA file made in Step 3.\n",
    "cated_fasta_path = 'test_op/cated.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1100006",
   "metadata": {},
   "source": [
    "* Set variable `total_amount` of the how many genomes participated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a19fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# directory path of output\n",
    "dereped_dir_path = 'test_op/dereped/'\n",
    "\n",
    "print(len(os.listdir(dereped_dir_path)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3791d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many genomes participated analysis.\n",
    "total_amount = len(os.listdir(dereped_dir_path)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd860a",
   "metadata": {},
   "source": [
    "#### run the codes below get final FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c2aea60",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 20 17:25:26 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qiime2-2022.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 20 17:25:30 2023\n"
     ]
    }
   ],
   "source": [
    "# mkdir or not\n",
    "if os.path.exists(output_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "cated_fasta = SeqIO.to_dict(SeqIO.parse(cated_fasta_path, \n",
    "                                        'fasta'\n",
    "                                       )\n",
    "                           )\n",
    "# which to write\n",
    "types_to_write = {\n",
    "    'accessory_cluster': False,\n",
    "    'strict_core': False,\n",
    "    'surplus_core': False\n",
    "}\n",
    "for x in cluster_type.split(','):\n",
    "    if x == 'accessory':\n",
    "        types_to_write['accessory_cluster'] = True\n",
    "        if os.path.exists(os.path.join(output_path, 'accessory_cluster')):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(os.path.join(output_path, 'accessory_cluster'))\n",
    "    \n",
    "    elif x == 'strict':\n",
    "        types_to_write['strict_core'] = True\n",
    "        if os.path.exists(os.path.join(output_path, 'strict_core')):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(os.path.join(output_path, 'strict_core'))\n",
    "            \n",
    "    elif x == 'surplus':\n",
    "        types_to_write['surplus_core'] = True\n",
    "        if os.path.exists(os.path.join(output_path, 'surplus_core')):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(os.path.join(output_path, 'surplus_core'))\n",
    "        \n",
    "    \n",
    "# read cluster         \n",
    "f = open(final_cluster_path, 'r')\n",
    "CLUSTERS = f.readlines()\n",
    "f.close()\n",
    "\n",
    "print(time.asctime())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mission_lst = [x.rstrip('\\n').split('\\t') for x in CLUSTERS]\n",
    "    \n",
    "    mission_lst = list(np.array_split(mission_lst, \n",
    "                                      process_number\n",
    "                                     )\n",
    "                      )\n",
    "    \n",
    "    # mp\n",
    "             \n",
    "    jobs = []\n",
    "    \n",
    "    for sub_mission_lst in mission_lst:\n",
    "        \n",
    "        p = Process(target = write_cluster_s,\n",
    "                    args = (sub_mission_lst,\n",
    "                            total_amount,\n",
    "                            types_to_write,\n",
    "                            output_path,\n",
    "                            cated_fasta\n",
    "                           )\n",
    "                   )\n",
    "            \n",
    "        p.start()\n",
    "        jobs.append(p)\n",
    "\n",
    "\n",
    "    for z in jobs:\n",
    "        z.join()\n",
    "        \n",
    "print(time.asctime())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
